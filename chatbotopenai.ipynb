{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0e836b90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e836b90",
        "outputId": "5e67475e-9631-4ca0-9d0a-1ea8fd23b595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.34)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro==1.8.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.8.2)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere\n",
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "\n",
        "#!git clone https://github.com/facebookresearch/faiss\n",
        "\n",
        "!pip install flask\n",
        "!pip install requests\n",
        "!pip install numpy\n",
        "!pip install faiss-cpu\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "78076c12",
      "metadata": {
        "id": "78076c12"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask import request, jsonify\n",
        "import numpy as np\n",
        "import openai\n",
        "import requests\n",
        "import faiss\n",
        "import os\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "_6reVzRAksII",
      "metadata": {
        "id": "_6reVzRAksII"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "xeBKB734lJ3y",
      "metadata": {
        "id": "xeBKB734lJ3y"
      },
      "outputs": [],
      "source": [
        "# Configura la clave de la API de OpenAI\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "141fbd28",
      "metadata": {
        "id": "141fbd28"
      },
      "outputs": [],
      "source": [
        "def generate_embedding(text, openai_api_key, chatbot_id):\n",
        "    # Genera un embedding para un texto dado utilizando OpenAI.\n",
        "    openai.api_key = 'your_openai_api_key' # Set your OpenAI API key here\n",
        "    response = openai.Embedding.create(engine=\"text-similarity-babbage-001\", input=text)\n",
        "    embedding = np.random.rand(768)  # Representación ficticia, reemplazar con la lógica adecuada\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "187ffca8",
      "metadata": {
        "id": "187ffca8"
      },
      "outputs": [],
      "source": [
        "def process_results(indices, data):\n",
        "    # Procesa los índices obtenidos de FAISS para recuperar información relevante.\n",
        "    info = \"Información relacionada con los índices en Milvus: \" + ', '.join(str(idx) for idx in indices)\n",
        "    return info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "125ec66d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "125ec66d",
        "outputId": "64878762-09a4-4234-da63-274271642b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índices de los vecinos más cercanos: [[-1 -1 -1 -1 -1]]\n",
            "Distancias de los vecinos más cercanos: [[3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38 3.4028235e+38]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Importar las bibliotecas necesarias\n",
        "# Suponiendo que los datos son embeddings de dimensión 768 (cambiar según sea necesario)\n",
        "dim = 768\n",
        "num_data_points = 10000  # Número de puntos de datos (cambiar según sea necesario)\n",
        "# Crear datos de ejemplo (reemplazar con tus propios datos)\n",
        "data = np.random.rand(num_data_points, dim).astype(np.float32)\n",
        "# Crear y entrenar el índice Faiss para la búsqueda de vecinos más cercanos\n",
        "index = faiss.IndexFlatL2(dim)  # Usar L2 para la distancia\n",
        "# Milvus adds data to the collection in a different way  # Agregar los datos al índice\n",
        "# Realizar una consulta de ejemplo\n",
        "query = np.random.rand(dim).astype(np.float32)\n",
        "k = 5  # Número de vecinos más cercanos a buscar\n",
        "distances, neighbors = index.search(query.reshape(1, -1), k)\n",
        "# Mostrar los resultados\n",
        "print(\"Índices de los vecinos más cercanos:\", neighbors)\n",
        "print(\"Distancias de los vecinos más cercanos:\", distances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7ec83592",
      "metadata": {
        "id": "7ec83592"
      },
      "outputs": [],
      "source": [
        "# Configuración para OpenAI\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7a9fb602",
      "metadata": {
        "id": "7a9fb602"
      },
      "outputs": [],
      "source": [
        "# Configuraciones\n",
        "UPLOAD_FOLDER = '/data/uploads/docs/'  # Ajusta esta ruta según sea necesario\n",
        "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'csv', 'docx', 'xlsx', 'pptx'}\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "# Esta ruta maneja la subida de archivos y almacena los embeddings en Milvus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b8bdc902",
      "metadata": {
        "id": "b8bdc902"
      },
      "outputs": [],
      "source": [
        "def allowed_file(filename, chatbot_id):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "# Esta función verifica si el archivo tiene una extensión permitida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a71ecdcf",
      "metadata": {
        "id": "a71ecdcf"
      },
      "outputs": [],
      "source": [
        "# Función para verificar si el archivo tiene una extensión permitida\n",
        "def allowed_file(filename, chatbot_id):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "Lkw5fLceFs-i",
      "metadata": {
        "id": "Lkw5fLceFs-i"
      },
      "outputs": [],
      "source": [
        "# Función para verificar si el archivo tiene una extensión permitida\n",
        "def allowed_file(filename, chatbot_id):\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xpXvxqn_OIHW"
      },
      "id": "xpXvxqn_OIHW"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#metodo param la subida de documentos\n",
        "\n",
        "@app.route('/uploads', methods=['POST'])\n",
        "def procesar_documento():\n",
        "    mensaje_palabras = \"\"\n",
        "    try:\n",
        "        # Recibiendo el archivo y el chatbot_id desde el formulario\n",
        "        if 'documento' not in request.files:\n",
        "            return jsonify({\"respuesta\": \"No se encontró el archivo 'documento'\", \"codigo_error\": 1})\n",
        "        file = request.files['documento']\n",
        "        chatbot_id = request.form['chatbot_id']\n",
        "\n",
        "        if file.filename == '':\n",
        "            return jsonify({\"respuesta\": \"No se seleccionó ningún archivo\", \"codigo_error\": 1})\n",
        "\n",
        "        # Guardar el archivo en el sistema de archivos\n",
        "        chatbot_folder = os.path.join(UPLOAD_FOLDER, str(chatbot_id))\n",
        "        os.makedirs(chatbot_folder, exist_ok=True)\n",
        "\n",
        "        destino = os.path.join(chatbot_folder, file.filename)\n",
        "        file.save(destino)\n",
        "\n",
        "        # Intentar contar las palabras en el documento\n",
        "        try:\n",
        "            with open(destino, 'r', encoding='utf-8') as f:\n",
        "                contenido = f.read()\n",
        "                numero_de_palabras = len(contenido.split())\n",
        "                mensaje_palabras = f\"Número de palabras en el documento: {numero_de_palabras}. \"\n",
        "        except Exception as e:\n",
        "            mensaje_palabras = \"No fue posible contar las palabras en el documento. Error: \" + str(e)\n",
        "\n",
        "        # Añadir documento a FAISS y a la base de datos\n",
        "        try:\n",
        "            # Aquí, 'documento' podría ser el nombre del archivo o una identificación única derivada de él\n",
        "            doc_id = file.filename  # O generar un ID único basado en 'file.filename'\n",
        "            add_document_to_faiss_and_db(contenido, doc_id)\n",
        "        except Exception as e:\n",
        "            mensaje_palabras += f\"No fue posible indexar el documento en FAISS: {e}. \"\n",
        "\n",
        "        mensaje_exito = \"Proceso completado con éxito. \" + mensaje_palabras\n",
        "        return jsonify({\"respuesta\": mensaje_exito, \"codigo_error\": 0})\n",
        "    except Exception as e:\n",
        "        mensaje_error = f\"Error: {str(e)}. \" + mensaje_palabras\n",
        "        return jsonify({\"respuesta\": mensaje_error, \"codigo_error\": 1})\n"
      ],
      "metadata": {
        "id": "Ifw3ryvdLuGQ"
      },
      "id": "Ifw3ryvdLuGQ",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "925abb83",
      "metadata": {
        "id": "925abb83"
      },
      "outputs": [],
      "source": [
        "\n",
        "@app.route('/fine-tuning', methods=['POST'])\n",
        "def fine_tuning():\n",
        "    # Obtener los datos del cuerpo de la solicitud\n",
        "    data = request.json\n",
        "    training_text = data.get('training_text')\n",
        "    chat_id = data.get('chat_id')\n",
        "\n",
        "    # Validación de los datos recibidos\n",
        "    if not training_text or not isinstance(chat_id, int):\n",
        "        return jsonify({\"status\": \"error\", \"message\": \"Invalid input\"}), 400\n",
        "\n",
        "    # Aquí puedes hacer algo con training_text y chat_id si es necesario\n",
        "\n",
        "    # Datos para el proceso de fine-tuning\n",
        "    training_data = {\n",
        "        # Suponiendo que estos son los datos que OpenAI necesita para el fine-tuning\n",
        "        \"text\": training_text,\n",
        "        \"chat_id\": chat_id\n",
        "    }\n",
        "\n",
        "    # Endpoint y headers para la API de OpenAI\n",
        "    openai_endpoint = \"https://api.openai.com/v1/models/fine-tune\"\n",
        "    headers = {\n",
        "        \"Authorization\": \"Bearer \" + openai.api_key\n",
        "    }\n",
        "\n",
        "    # Realizar la solicitud POST a la API de OpenAI\n",
        "    response = requests.post(openai_endpoint, json=training_data, headers=headers)\n",
        "\n",
        "    # Manejar la respuesta\n",
        "    if response.status_code == 200:\n",
        "        return jsonify({\"status\": \"fine-tuning started\", \"response\": response.json()})\n",
        "    else:\n",
        "        return jsonify({\"status\": \"error\", \"message\": response.text}), response.status_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "8455c9fd",
      "metadata": {
        "id": "8455c9fd"
      },
      "outputs": [],
      "source": [
        "@app.route('/save_urls', methods=['POST'])\n",
        "def save_urls(chatbot_id):\n",
        "    data = request.json\n",
        "    urls = data.get('urls', [])\n",
        "    chatbot_id = data.get('chatbot_id')\n",
        "    if not chatbot_id or len(urls) == 0:\n",
        "        return jsonify({\"status\": \"error\", \"message\": \"No chatbot_id or URLs provided\"}), 400\n",
        "    file_path = os.path.join('uploads/scraping', f'{chatbot_id}.txt')\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "    with open(file_path, 'a') as file:\n",
        "        for url in urls:\n",
        "            file.write(url + '\\n')\n",
        "    return jsonify({\"status\": \"success\", \"message\": \"URLs saved successfully\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e7030a3e",
      "metadata": {
        "id": "e7030a3e"
      },
      "outputs": [],
      "source": [
        "@app.route('/process_urls', methods=['POST'])\n",
        "def process_urls():\n",
        "    data = request.json\n",
        "    chatbot_id = data.get('chatbot_id')\n",
        "    if not chatbot_id:\n",
        "        return jsonify({\"status\": \"error\", \"message\": \"No chatbot_id provided\"}), 400\n",
        "\n",
        "    file_path = os.path.join('uploads/scraping', f'{chatbot_id}.txt')\n",
        "    if not os.path.exists(file_path):\n",
        "        return jsonify({\"status\": \"error\", \"message\": \"File not found\"}), 404\n",
        "\n",
        "    results = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        urls = file.readlines()\n",
        "\n",
        "    for url in urls:\n",
        "        url = url.strip()\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            text = soup.get_text().split()\n",
        "            word_count = len(text)\n",
        "\n",
        "            # Añadir el contenido de la URL a FAISS y a la base de datos\n",
        "            # Aquí, el 'url' se usa como identificador único\n",
        "            try:\n",
        "                add_document_to_faiss_and_db(' '.join(text), url)\n",
        "                results.append({\"url\": url, \"word_count\": word_count, \"indexed\": True})\n",
        "            except Exception as e:\n",
        "                results.append({\"url\": url, \"word_count\": word_count, \"indexed\": False, \"index_error\": str(e)})\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            results.append({\"url\": url, \"error\": str(e)})\n",
        "\n",
        "    return jsonify({\"status\": \"success\", \"urls\": results})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d75d144f",
      "metadata": {
        "id": "d75d144f"
      },
      "outputs": [],
      "source": [
        "#Recibimos las urls no validas de front, de cicerone\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/invalid_urls', methods=['POST'])\n",
        "def handle_invalid_urls():\n",
        "    data = request.json\n",
        "\n",
        "    # Parámetros existentes\n",
        "    urls = data.get('urls', [])\n",
        "    chatbot_id = data.get('chatbot_id')\n",
        "\n",
        "    # Nuevos parámetros\n",
        "    long_text = data.get('long_text')\n",
        "    chat_id = data.get('chat_id')\n",
        "\n",
        "    # Verificar si se proporcionaron todos los datos necesarios\n",
        "    if not chatbot_id or len(urls) == 0 or long_text is None or chat_id is None:\n",
        "        return jsonify({\"status\": \"error\", \"message\": \"Missing required parameters\"}), 400\n",
        "\n",
        "    # Aquí puedes hacer algo con long_text y chat_id\n",
        "\n",
        "    file_path = os.path.join('uploads/scraping', f'{chatbot_id}.txt')\n",
        "\n",
        "    # Verificar si el archivo existe\n",
        "    if not os.path.exists(file_path):\n",
        "        return jsonify({\"status\": \"error\", \"message\": \"File not found\"}), 404\n",
        "\n",
        "    try:\n",
        "        # Leer las URLs actuales del archivo\n",
        "        with open(file_path, 'r') as file:\n",
        "            existing_urls = file.readlines()\n",
        "\n",
        "        # Filtrar las URLs para eliminar las proporcionadas\n",
        "        updated_urls = [url for url in existing_urls if url.strip() not in urls]\n",
        "\n",
        "        # Guardar las URLs actualizadas en el archivo\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.writelines(updated_urls)\n",
        "\n",
        "        return jsonify({\n",
        "            \"status\": \"success\",\n",
        "            \"message\": \"URLs have been updated\",\n",
        "            \"removed_urls\": urls\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c7993d5e",
      "metadata": {
        "id": "c7993d5e"
      },
      "outputs": [],
      "source": [
        "# Función para eliminar URLs\n",
        "\n",
        "@app.route('/delete_urls', methods=['POST'])\n",
        "def delete_urls():\n",
        "    data = request.json\n",
        "    chatbot_id = data.get('chatbot_id')\n",
        "    urls_to_delete = data.get('urls_to_delete', [])\n",
        "\n",
        "    # Verificar si se proporcionaron los datos necesarios\n",
        "    if not chatbot_id or len(urls_to_delete) == 0:\n",
        "        return \"Faltan el chatbot_id o las URLs para eliminar.\", 400\n",
        "\n",
        "    file_path = f'uploads/scraping/{chatbot_id}.txt'  # Ajustar según la estructura real de archivos\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            urls = file.read().splitlines()\n",
        "\n",
        "        updated_urls = [url for url in urls if url not in urls_to_delete]\n",
        "\n",
        "        with open(file_path, 'w') as file:\n",
        "            for url in updated_urls:\n",
        "                file.write(url + '\\n')\n",
        "\n",
        "        return \"Las URLs se han eliminado correctamente.\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"No se encontró el archivo para el chatbot_id '{chatbot_id}'.\", 404\n",
        "    except Exception as e:\n",
        "        return f\"Ocurrió un error durante la eliminación: {e}\", 500\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "39d91c6a",
      "metadata": {
        "id": "39d91c6a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Supongamos que ya tenemos un índice FAISS y funciones para generar embeddings y procesar resultados\n",
        "# index = ... (índice FAISS ya creado)\n",
        "# generate_embedding = ... (función para generar embeddings a partir de texto)\n",
        "# process_results = ... (función para procesar los índices de FAISS y obtener información relevante)\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    try:\n",
        "        # Recibir la consulta de texto\n",
        "        query_text = request.json.get('query')\n",
        "\n",
        "        # Generar un embedding para la consulta usando OpenAI\n",
        "        query_embedding = generate_embedding(query_text)\n",
        "\n",
        "        # Realizar la búsqueda en FAISS\n",
        "        k = 5  # Número de resultados a devolver\n",
        "        distances, indices = index.search(np.array([query_embedding]).astype(np.float32), k)\n",
        "\n",
        "        # Procesar los índices para obtener la información correspondiente\n",
        "        info = process_results(indices)\n",
        "\n",
        "        # Utilizar OpenAI para generar una respuesta comprensible en español\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",  # Especifica el modelo de OpenAI a utilizar\n",
        "            prompt=info,\n",
        "            max_tokens=150,  # Define el número máximo de tokens en la respuesta\n",
        "            temperature=0.7,  # Ajusta la creatividad de la respuesta\n",
        "            top_p=1,  # Controla la diversidad de la respuesta\n",
        "            frequency_penalty=0,  # Penalización por frecuencia de uso de palabras\n",
        "            presence_penalty=0,  # Penalización por presencia de palabras\n",
        "            language=\"es\"  # Especifica el idioma de la respuesta\n",
        "        )\n",
        "\n",
        "        # Extraer el texto de la respuesta\n",
        "        response_text = response.choices[0].text.strip()\n",
        "\n",
        "        # Devolver la respuesta\n",
        "        return jsonify({\"response\": response_text})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "55f00e5d",
      "metadata": {
        "id": "55f00e5d"
      },
      "outputs": [],
      "source": [
        "# Supongamos que estas son tus funciones para generar embeddings y manejar FAISS\n",
        "def generate_embeddings(data, chatbot_id):\n",
        "    # Esta función debe generar embeddings para tus datos usando el modelo de OpenAI\n",
        "    # Aquí hay un placeholder, reemplázalo con tu lógica específica\n",
        "    return [np.random.random(512) for _ in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "6892496b",
      "metadata": {
        "id": "6892496b"
      },
      "outputs": [],
      "source": [
        "def update_faiss_index(embeddings, chatbot_id):\n",
        "    # Esta función debe actualizar el índice de FAISS con nuevos embeddings\n",
        "    index = faiss.IndexFlatL2(512)  # Suponiendo que usas un índice FlatL2\n",
        "    index.add(np.array(embeddings).astype(np.float32))\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c1fca788",
      "metadata": {
        "id": "c1fca788"
      },
      "outputs": [],
      "source": [
        "@app.route('/fine-tuning', methods=['POST'])\n",
        "def fine_tune_model(chatbot_id):\n",
        "    training_data = request.json\n",
        "    # Envío de datos a la API de OpenAI para el proceso de fine-tuning\n",
        "    openai_endpoint = \"https://api.openai.com/v1/models/fine-tune\"\n",
        "    headers = {\"Authorization\": \"Bearer your_api_key_here\"}\n",
        "    response = requests.post(openai_endpoint, json=training_data, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        # Aquí, inicia el proceso de reentrenamiento de FAISS\n",
        "        embeddings = generate_embeddings(training_data)  # Genera nuevos embeddings\n",
        "        updated_index = update_faiss_index(embeddings)  # Actualiza el índice de FAISS\n",
        "        return jsonify({\"status\": \"fine-tuning started\", \"FAISS index updated\": True, \"response\": response.json()})\n",
        "    else:\n",
        "        return jsonify({\"status\": \"error\", \"message\": response.text})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "d9d5a8e3",
      "metadata": {
        "id": "d9d5a8e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0abf660-58d0-4e5a-a3c3-187740bd029e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de FAISS: 1.7.4\n",
            "Atributos y métodos en FAISS: ['APPROX_TOPK_BUCKETS_B16_D2', 'APPROX_TOPK_BUCKETS_B32_D2', 'APPROX_TOPK_BUCKETS_B8_D2', 'APPROX_TOPK_BUCKETS_B8_D3', 'AdditiveCoarseQuantizer', 'AdditiveQuantizer', 'AlignedTableFloat32', 'AlignedTableFloat32_round_capacity', 'AlignedTableUint16', 'AlignedTableUint16_round_capacity', 'AlignedTableUint8', 'AlignedTableUint8_round_capacity', 'AlignedTable_to_array', 'ArrayInvertedLists', 'AutoTuneCriterion', 'BinaryInvertedListScanner', 'BitstringReader', 'BitstringWriter', 'BlockInvertedLists', 'BufferList', 'BufferedIOReader', 'BufferedIOWriter', 'ByteVector', 'ByteVectorVector', 'CMax_float_partition_fuzzy', 'CMax_uint16_partition_fuzzy', 'CMin_float_partition_fuzzy', 'CMin_uint16_partition_fuzzy', 'CenteringTransform', 'CharVector', 'Cloner', 'Clustering', 'Clustering1D', 'ClusteringIterationStats', 'ClusteringIterationStatsVector', 'ClusteringParameters', 'CodePacker', 'CodePackerFlat', 'ConcatenatedInvertedLists', 'DirectMap', 'DirectMapAdd', 'DistanceComputer', 'DoubleVector', 'EXACT_TOPK', 'EnumeratedVectors', 'FAISS_VERSION_MAJOR', 'FAISS_VERSION_MINOR', 'FAISS_VERSION_PATCH', 'FastScanStats', 'FileIOReader', 'FileIOWriter', 'FlatCodesDistanceComputer', 'Float32Vector', 'Float32VectorVector', 'Float64Vector', 'FloatVector', 'FloatVectorVector', 'HNSW', 'HNSWStats', 'HNSW_shrink_neighbor_list', 'HStackInvertedLists', 'IDSelector', 'IDSelectorAll', 'IDSelectorAnd', 'IDSelectorArray', 'IDSelectorBatch', 'IDSelectorBitmap', 'IDSelectorNot', 'IDSelectorOr', 'IDSelectorRange', 'IDSelectorXOr', 'IOReader', 'IOWriter', 'IO_FLAG_MMAP', 'IO_FLAG_ONDISK_SAME_DIR', 'IO_FLAG_READ_ONLY', 'IO_FLAG_SKIP_IVF_DATA', 'IO_FLAG_SKIP_PRECOMPUTE_TABLE', 'ITQMatrix', 'ITQTransform', 'IVFFastScanStats', 'IVFPQSearchParameters', 'IVFSearchParameters', 'IcmEncoder', 'IcmEncoderFactory', 'Index', 'Index2Layer', 'IndexAdditiveQuantizer', 'IndexAdditiveQuantizerFastScan', 'IndexBinary', 'IndexBinaryFlat', 'IndexBinaryFromFloat', 'IndexBinaryHNSW', 'IndexBinaryHash', 'IndexBinaryHashStats', 'IndexBinaryIDMap', 'IndexBinaryIDMap2', 'IndexBinaryIVF', 'IndexBinaryMultiHash', 'IndexBinaryReplicas', 'IndexBinaryShards', 'IndexFastScan', 'IndexFlat', 'IndexFlat1D', 'IndexFlatCodes', 'IndexFlatIP', 'IndexFlatL2', 'IndexHNSW', 'IndexHNSW2Level', 'IndexHNSWFlat', 'IndexHNSWPQ', 'IndexHNSWSQ', 'IndexIDMap', 'IndexIDMap2', 'IndexIVF', 'IndexIVFAdditiveQuantizer', 'IndexIVFAdditiveQuantizerFastScan', 'IndexIVFFastScan', 'IndexIVFFlat', 'IndexIVFFlatDedup', 'IndexIVFInterface', 'IndexIVFLocalSearchQuantizer', 'IndexIVFLocalSearchQuantizerFastScan', 'IndexIVFPQ', 'IndexIVFPQFastScan', 'IndexIVFPQR', 'IndexIVFPQStats', 'IndexIVFProductLocalSearchQuantizer', 'IndexIVFProductLocalSearchQuantizerFastScan', 'IndexIVFProductResidualQuantizer', 'IndexIVFProductResidualQuantizerFastScan', 'IndexIVFResidualQuantizer', 'IndexIVFResidualQuantizerFastScan', 'IndexIVFScalarQuantizer', 'IndexIVFSpectralHash', 'IndexIVFStats', 'IndexLSH', 'IndexLattice', 'IndexLocalSearchQuantizer', 'IndexLocalSearchQuantizerFastScan', 'IndexNNDescent', 'IndexNNDescentFlat', 'IndexNSG', 'IndexNSGFlat', 'IndexNSGPQ', 'IndexNSGSQ', 'IndexPQ', 'IndexPQFastScan', 'IndexPQStats', 'IndexPreTransform', 'IndexProductLocalSearchQuantizer', 'IndexProductLocalSearchQuantizerFastScan', 'IndexProductResidualQuantizer', 'IndexProductResidualQuantizerFastScan', 'IndexProxy', 'IndexRandom', 'IndexRefine', 'IndexRefineFlat', 'IndexReplicas', 'IndexResidual', 'IndexResidualQuantizer', 'IndexResidualQuantizerFastScan', 'IndexRowwiseMinMax', 'IndexRowwiseMinMaxBase', 'IndexRowwiseMinMaxFP16', 'IndexScalarQuantizer', 'IndexShards', 'IndexShardsIVF', 'IndexSplitVectors', 'Int16Vector', 'Int32Vector', 'Int32VectorVector', 'Int64Vector', 'Int64VectorVector', 'Int8Vector', 'IntVector', 'InterruptCallback', 'InterruptCallback_check', 'InterruptCallback_clear_instance', 'InterruptCallback_get_period_hint', 'InterruptCallback_is_interrupted', 'IntersectionCriterion', 'InvertedListScanner', 'InvertedLists', 'InvertedListsIOHook', 'InvertedListsIOHook_add_callback', 'InvertedListsIOHook_lookup', 'InvertedListsIOHook_lookup_classname', 'InvertedListsIOHook_print_callbacks', 'InvertedListsIterator', 'InvertedListsPtrVector', 'Kmeans', 'LSQTimer', 'LSQTimerScope', 'Level1Quantizer', 'LinearTransform', 'LocalSearchCoarseQuantizer', 'LocalSearchQuantizer', 'LongLongVector', 'LongVector', 'LongVectorVector', 'LooseVersion', 'METRIC_BrayCurtis', 'METRIC_Canberra', 'METRIC_INNER_PRODUCT', 'METRIC_Jaccard', 'METRIC_JensenShannon', 'METRIC_L1', 'METRIC_L2', 'METRIC_Linf', 'METRIC_Lp', 'MapLong2Long', 'MaskedInvertedLists', 'MatrixStats', 'MultiIndexQuantizer', 'MultiIndexQuantizer2', 'NNDescent', 'NSG', 'Neighbor', 'Nhood', 'NormalizationTransform', 'OPQMatrix', 'OnDiskInvertedLists', 'OnDiskOneList', 'OnDiskOneListVector', 'OneRecallAtRCriterion', 'OperatingPoint', 'OperatingPointVector', 'OperatingPoints', 'PCAMatrix', 'PQDecoder16', 'PQDecoder8', 'PQDecoderGeneric', 'PQEncoder16', 'PQEncoder8', 'PQEncoderGeneric', 'ParameterRange', 'ParameterRangeVector', 'ParameterSpace', 'PartitionStats', 'PermutationObjective', 'PolysemousTraining', 'ProductAdditiveQuantizer', 'ProductLocalSearchQuantizer', 'ProductQuantizer', 'ProductResidualQuantizer', 'ProgressiveDimClustering', 'ProgressiveDimClusteringParameters', 'ProgressiveDimIndexFactory', 'PyCallbackIDSelector', 'PyCallbackIOReader', 'PyCallbackIOWriter', 'Quantizer', 'RandomGenerator', 'RandomRotationMatrix', 'RangeQueryResult', 'RangeSearchPartialResult', 'RangeSearchPartialResult_merge', 'RangeSearchResult', 'ReadOnlyInvertedLists', 'ReconstructFromNeighbors', 'RemapDimensionsTransform', 'Repeat', 'RepeatVector', 'Repeats', 'ReproduceDistancesObjective', 'ReproduceDistancesObjective_compute_mean_stdev', 'ReproduceDistancesObjective_sqr', 'ResidualCoarseQuantizer', 'ResidualQuantizer', 'ResultHeap', 'SHARED_PTR_DISOWN', 'ScalarQuantizer', 'SearchParameters', 'SearchParametersHNSW', 'SearchParametersIVF', 'SearchParametersPQ', 'SearchParametersPreTransform', 'SearchParametersResidualCoarseQuantizer', 'SimulatedAnnealingOptimizer', 'SimulatedAnnealingParameters', 'SliceInvertedLists', 'SlidingIndexWindow', 'StopWordsInvertedLists', 'SwigPyIterator', 'ThreadedIndexBase', 'ThreadedIndexBaseBinary', 'UInt16Vector', 'UInt32Vector', 'UInt64Vector', 'UInt8Vector', 'UInt8VectorVector', 'Uint64Vector', 'VStackInvertedLists', 'VectorIOReader', 'VectorIOWriter', 'VectorTransform', 'VectorTransformVector', 'VisitedTable', 'ZnSphereCodec', 'ZnSphereCodecAlt', 'ZnSphereCodecRec', 'ZnSphereSearch', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_swigfaiss_avx2', 'add_ref_in_constructor', 'add_ref_in_function', 'add_ref_in_method', 'add_ref_in_method_explicit_own', 'add_to_referenced_objects', 'aq_estimate_norm_scale', 'aq_quantize_LUT_and_bias', 'array', 'array_conversions', 'array_to_AlignedTable', 'base_prefix', 'beam_search_encode_step', 'beam_search_encode_step_tab', 'binary_to_real', 'bincode_hist', 'bitvec_print', 'bitvec_shuffle', 'bitvecs2fvecs', 'bucket_sort', 'byte_rand', 'cast_integer_to_float_ptr', 'cast_integer_to_idx_t_ptr', 'cast_integer_to_int_ptr', 'cast_integer_to_uint8_ptr', 'cast_integer_to_void_ptr', 'check_compatible_for_merge', 'check_openmp', 'checksum', 'class_wrappers', 'clone_Quantizer', 'clone_index', 'compute_PQ_dis_tables_dsub2', 'copy_array_to_AlignedTable', 'copy_array_to_vector', 'crosshamming_count_thres', 'cvar', 'depr_prefix', 'deprecated_name_map', 'deserialize_index', 'deserialize_index_binary', 'downcast_AdditiveQuantizer', 'downcast_IndexBinary', 'downcast_InvertedLists', 'downcast_Quantizer', 'downcast_VectorTransform', 'downcast_index', 'eval_intersection', 'extra_wrappers', 'extract_index_ivf', 'float_maxheap_array_t', 'float_minheap_array_t', 'float_rand', 'float_randn', 'fourcc', 'fourcc_inv', 'fourcc_inv_printable', 'fvec2bitvec', 'fvec_L1', 'fvec_L2sqr', 'fvec_L2sqr_by_idx', 'fvec_L2sqr_ny', 'fvec_L2sqr_ny_nearest', 'fvec_L2sqr_ny_nearest_y_transposed', 'fvec_L2sqr_ny_transposed', 'fvec_Linf', 'fvec_add', 'fvec_argsort', 'fvec_argsort_parallel', 'fvec_inner_product', 'fvec_inner_products_by_idx', 'fvec_inner_products_ny', 'fvec_madd', 'fvec_madd_and_argmin', 'fvec_norm_L2sqr', 'fvec_norms_L2', 'fvec_norms_L2sqr', 'fvec_renorm_L2', 'fvec_sub', 'fvecs2bitvecs', 'fvecs_maybe_subsample', 'generalized_hammings_knn_hc', 'get_compile_options', 'get_cycles', 'get_extra_distance_computer', 'get_invlist_range', 'get_mem_usage_kb', 'get_num_gpus', 'getmillisecs', 'gpu_profiler_start', 'gpu_profiler_stop', 'gpu_sync_all_devices', 'gpu_wrappers', 'hamdis_tab_ham_bytes', 'hamming_count_thres', 'hamming_range_search', 'hammings', 'hammings_knn', 'hammings_knn_hc', 'hammings_knn_mc', 'has_AVX2', 'hash_bytes', 'imbalance_factor', 'index_binary_factory', 'index_cpu_to_all_gpus', 'index_cpu_to_gpu_multiple_py', 'index_cpu_to_gpus_list', 'index_factory', 'initialize_IVFPQ_precomputed_table', 'inner_product_to_L2sqr', 'inspect', 'int64_rand', 'int64_rand_max', 'int_maxheap_array_t', 'int_minheap_array_t', 'is_similarity_metric', 'ivec_checksum', 'ivec_hist', 'ivf_residual_add_from_flat_codes', 'ivf_residual_from_quantizer', 'kmax', 'kmeans1d', 'kmeans_clustering', 'kmin', 'knn', 'knn_L2sqr', 'knn_L2sqr_by_idx', 'knn_gpu', 'knn_inner_product', 'knn_inner_products_by_idx', 'lo_build', 'lo_listno', 'lo_offset', 'loader', 'logger', 'logging', 'lrand', 'match_hamming_thres', 'matrix_bucket_sort_inplace', 'matrix_qr', 'memcpy', 'merge_into', 'merge_knn_results', 'merge_knn_results_CMax', 'merge_knn_results_CMin', 'merge_result_table_with', 'normalize_L2', 'np', 'obj', 'omp_get_max_threads', 'omp_set_num_threads', 'os', 'pairwise_L2sqr', 'pairwise_distance_gpu', 'pairwise_distances', 'pairwise_extra_distances', 'pairwise_indexed_L2sqr', 'pairwise_indexed_inner_product', 'platform', 'popcount32', 'popcount64', 'quantize_LUT_and_bias', 'rand', 'rand_perm', 'rand_smooth_vectors', 'randint', 'randn', 'range_search_L2sqr', 'range_search_inner_product', 'range_search_with_parameters', 'range_search_with_parameters_c', 'ranklist_handle_ties', 'ranklist_intersection_size', 'read_InvertedLists', 'read_ProductQuantizer', 'read_VectorTransform', 'read_index', 'read_index_binary', 'real_to_binary', 'reflection', 'rev_swig_ptr', 'round_uint8_per_column', 'round_uint8_per_column_multi', 'search_and_return_centroids', 'search_centroid', 'search_with_parameters', 'search_with_parameters_c', 'serialize_index', 'serialize_index_binary', 'set_invlist_range', 'simd_histogram_16', 'simd_histogram_8', 'sizeof_long', 'smawk', 'storage_distance_computer', 'subprocess', 'supported_instruction_sets', 'swig_ptr', 'swigfaiss_avx2', 'symbol', 'sys', 'the_class', 'this_module', 'try_extract_index_ivf', 'vector_float_to_array', 'vector_name_map', 'vector_to_array', 'wait', 'warnings', 'write_InvertedLists', 'write_ProductQuantizer', 'write_VectorTransform', 'write_index', 'write_index_binary']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Código para diagnosticar problemas con FAISS\n",
        "\n",
        "import faiss\n",
        "\n",
        "# Verificar la versión de FAISS\n",
        "print('Versión de FAISS:', faiss.__version__)\n",
        "\n",
        "# Listar los atributos y métodos de FAISS\n",
        "print('Atributos y métodos en FAISS:', dir(faiss))\n",
        "\n",
        "# Si necesitas reinstalar FAISS, descomenta las siguientes líneas:\n",
        "# !pip uninstall -y faiss faiss-gpu faiss-cpu\n",
        "# !pip install faiss-cpu  # Para CPU\n",
        "# !pip install faiss-gpu  # Para GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "8b0b9096",
      "metadata": {
        "id": "8b0b9096"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Suponiendo que tienes un diccionario para mapear IDs de documentos a índices en FAISS\n",
        "doc_id_to_faiss_index = {}\n",
        "\n",
        "def add_document_to_faiss(document, doc_id):\n",
        "    # Verifica si el documento ya está indexado\n",
        "    if doc_id in doc_id_to_faiss_index:\n",
        "        print(f\"Documento con ID {doc_id} ya indexado.\")\n",
        "        return\n",
        "\n",
        "    # Genera el embedding para el nuevo documento\n",
        "    embedding = generate_embedding(document)\n",
        "\n",
        "    # Añade el embedding al índice de FAISS y actualiza el mapeo\n",
        "    faiss_index = get_faiss_index()  # Obtén tu índice de FAISS actual\n",
        "    faiss_index.add(np.array([embedding]))  # Añade el nuevo embedding\n",
        "    new_index = faiss_index.ntotal - 1  # El nuevo índice en FAISS\n",
        "    doc_id_to_faiss_index[doc_id] = new_index  # Actualiza el mapeo\n",
        "\n",
        "    # No olvides guardar el índice de FAISS y el mapeo actualizado de forma persistente\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "      app.run(debug=True, port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji9e1zYrPeyA",
        "outputId": "ee463b59-8bec-46b7-f807-852608f1f60d"
      },
      "id": "ji9e1zYrPeyA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}